# Code base reused from https://github.com/benedekrozemberczki/graph2vec

import argparse, os, logging, time
from train_utils import train_skipgram
from make_corpus import *
from time import time
import random
import shutil
import csv

from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from gensim.models.callbacks import CallbackAny2Vec
from visualize_embeddings import visualize_embeddings
from utils import *

logger = logging.getLogger()
logger.setLevel("INFO")

DUP_SUFFIX = "#2"


def main(args):
    corpus_dir = args.corpus
    output_dir = args.output_dir
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    batch_size = args.batch_size
    epochs = args.epochs
    embedding_size = args.embedding_size
    num_negsample = args.num_negsample
    learning_rate = args.learning_rate
    suffix = args.suffix
    label_filed_name = "Label"
    gexf_extn = "vocabulary"

    assert os.path.exists(corpus_dir), "File {} does not exist".format(corpus_dir)
    assert os.path.exists(output_dir), "Dir {} does not exist".format(output_dir)

    graph_files = get_files(dirname=corpus_dir, extn=".gexf", max_files=0)

    t0 = time()
    vocabulary_params = {
        "wlk_sizes": args.wlk_sizes,
        "individual_nodes": args.augment_individual_nodes,
        "neighborhoods": args.augment_neighborhoods,
        "non_adjacent_pairs": args.augment_pairwise_relations,
        "mutually_exclusive_pairs": args.augment_mutually_exclusive_relations,
        "direct_edges": args.augment_direct_edges,
        "structure": args.augment_tree_structure,
        "root_child_relations": args.augment_root_child_relations,
        "root_label": args.root_label,
        "ignore_label": args.ignore_label,
        "remove_unique_words": args.remove_unique_words,
    }

    t0 = time()
    timestamp = str(int(t0))
    filename_prefix = "_".join([timestamp, os.path.basename(corpus_dir)])

    dir_path = os.path.join(args.output_dir, filename_prefix)
    os.makedirs(dir_path)

    output_files_prefix = timestamp + suffix
    embeddings_fname = output_files_prefix + "_embeddings.csv"
    embeddings_path = os.path.join(dir_path, embeddings_fname)

    training_params = vocabulary_params
    training_params["embedding_size"] = embedding_size
    training_params["epochs"] = epochs
    training_params["learning_rate"] = learning_rate
    training_params["wlk_h"] = max(args.wlk_sizes)
    training_params["num_negsample"] = num_negsample
    with open(
        os.path.join(dir_path, output_files_prefix + "_training_params.txt"), "w"
    ) as filehandle:
        json.dump(training_params, filehandle, indent=2)

    treename_mapping_file = args.filename_samplename_mapping
    if not treename_mapping_file:
        treename_mapping_file = os.path.join(corpus_dir + "filename_index.csv")

    if os.path.isfile(treename_mapping_file):
        treename_mapping = (
            pd.read_csv(treename_mapping_file, header=None, index_col=0, dtype=str)
            .squeeze("columns")
            .to_dict()
        )
    else:
        treename_mapping = {}
        for path in graph_files:
            sample_name = path2name(path)
            treename_mapping[sample_name] = sample_name

    # Add dulicates if not generated by a previous iteration.
    dup_paths = [x for x in graph_files if DUP_SUFFIX in x]
    if not dup_paths:
        for path in graph_files:
            dir_name = os.path.dirname(path)
            basename = path2name(path)
            _, extension = os.path.splitext(path)
            dup_path = os.path.join(dir_name, basename + DUP_SUFFIX + extension)
            shutil.copyfile(path, dup_path)
            dup_paths.append(dup_path)
        graph_files.extend(dup_paths)

        for dup_path in dup_paths:
            dup_sample_name = path2name(dup_path)
            sample_name = dup_sample_name.removesuffix(DUP_SUFFIX)
            treename_mapping[dup_sample_name] = (
                treename_mapping[sample_name] + DUP_SUFFIX
            )
    with open(os.path.join(corpus_dir, "filename_index.csv"), "w") as file:
        w = csv.writer(file)
        w.writerows(treename_mapping.items())

    anytrees = gexf_to_anytree(graph_files, treename_mapping)
    with open(os.path.join(corpus_dir, "trees.json"), "w") as file:
        file.write(json.dumps(anytrees))

    if args.build_vocabulary:
        build_tree_vocabulary(
            graph_files,
            max_h=max(args.wlk_sizes),
            vocabulary_params=vocabulary_params,
            node_label_attr_name=label_filed_name,
            word_tag=args.word_tag,
        )
        logging.info("dumped sg2vec sentences in {} sec.".format(time() - t0))


    if args.use_package:

        def save_embeddings(output_path, model, files, tree_mapping, dimensions):
            """
            Function to save the embedding.
            :param output_dir: Path to the output directory.
            :param model: The embedding model object.
            :param files: The list of files.
            :param dimensions: The embedding dimension parameter.
            """
            out = []
            indices = []
            for f in files:
                identifier = path2name(f)
                if not identifier[0].isdigit():
                    continue
                out.append(list(model.dv["g_" + identifier]))
                indices.append(identifier)
            column_names = [str(dim) for dim in range(dimensions)]
            out = pd.DataFrame(out, index=indices, columns=column_names)
            out.rename(index=tree_mapping, inplace=True)
            out.index.name = "graphs"
            out.columns.name = "features"
            out.to_csv(output_path)
            return out

        feature_files = get_files(
            dirname=corpus_dir, extn=".gexf." + gexf_extn, max_files=0
        )
        documents = []
        for filename in feature_files:
            file = open(filename, "r")
            file_index = filename.split("/")[-1].split(".")[0]
            file_vocabulary = file.read().split("\n")
            file_vocabulary.remove("")
            documents.append(
                TaggedDocument(words=file_vocabulary, tags=["g_" + str(file_index)])
            )

        class callback(CallbackAny2Vec):
            """Callback to print loss after each epoch."""

            def __init__(self):
                self.epoch = 0

            def on_epoch_end(self, model):
                loss = model.get_latest_training_loss()
                print("Loss after epoch {}: {}".format(self.epoch, loss))
                self.epoch += 1

        model = Doc2Vec(
            documents,
            vector_size=args.embedding_size,
            window=2,  # the maximum distance between the current and predicted word within a sentence.
            min_count=0,  # args.min_count,
            dm=0,  # if 1, distributed memory (PV-DM) is used; otherwise, distributed bag of words (PV-DBOW) is employed
            dbow_words=1,  # trains word-vectors (in skip-gram fashion) simultaneous with DBOW doc-vector training
            seed=random.randint(0, 10000000),
            sample=0,  # args.down_sampling,
            workers=4,  # args.workers,
            epochs=epochs,
            alpha=learning_rate,
            # min_alpha = 0.1,
            hs=0,  # if set to 0, and negative is non-zero, negative sampling will be used.
            negative=num_negsample,
            compute_loss=True,
            callbacks=[callback()],
        )

        print("Final loss:", model.get_latest_training_loss())

        graphs = glob.glob(os.path.join(corpus_dir, "*.gexf"))
        df_embeddings = save_embeddings(
            embeddings_path, model, graphs, treename_mapping, embedding_size
        )

    else:
        df_embeddings = train_skipgram(
            corpus_dir,
            treename_mapping,
            gexf_extn,
            learning_rate,
            embedding_size,
            num_negsample,
            epochs,
            batch_size,
            os.path.join(dir_path, output_files_prefix + "_"),
            args.generate_heatmaps,
        )
    logging.info("Trained the skipgram model in {} sec.".format(round(time() - t0, 2)))

    # Visualize embeddings.
    command = (
        "python visualize_embeddings.py --in_embeddings "
        + os.path.join(
            dir_path,
            "_".join([output_files_prefix, "iter" + str(epochs) + "embeddings.csv"]),
        )
        + " --corpus_dir"
        + corpus_dir
        + " --threshold 0.5"
    )
    print()
    print("*** Command for generating custom embedding visualizations ***")
    print(command)
    print()


def parse_args():
    args = argparse.ArgumentParser("oncotree2vec")
    args.add_argument(
        "-c",
        "--corpus",
        required=True,
        help="Path to directory containing tree files in GEXF format to be used for  clustering",
    )

    args.add_argument(
        "-f",
        "--filename_samplename_mapping",
        help="Path to csv file containing the mapping between the GEXF filenames and the tree sample names.",
    )

    args.add_argument(
        "-o",
        "--output_dir",
        default="../embeddings",
        help="Path to directory for storing output embeddings",
    )

    args.add_argument(
        "-b",
        "--batch_size",
        default=128,
        type=int,
        help="Number of samples per training batch",
    )

    args.add_argument(
        "-e",
        "--epochs",
        default=1000,
        type=int,
        help="Number of iterations the whole dataset of trees is traversed",
    )

    args.add_argument(
        "-d",
        "--embedding_size",
        default=1024,
        type=int,
        help="Intended tree embedding size to be learnt",
    )

    args.add_argument(
        "-neg",
        "--num_negsample",
        default=10,
        type=int,
        help="Number of negative samples to be used for training",
    )

    args.add_argument(
        "-lr",
        "--learning_rate",
        default=0.3,
        type=float,
        help="Learning rate to optimize the loss function",
    )

    args.add_argument(
        "--wlk_sizes",
        nargs="*",
        default=[0, 1, 2, 3],
        type=int,
        help="Seizes of WL kernel (i.e., degree of rooted subtree "
        "features to be considered for representation learning)",
    )

    args.add_argument(
        "-s",
        "--suffix",
        default="",
        help="Suffix to be added to the output filenames",
    )

    args.add_argument(
        "-x0",
        "--augment_individual_nodes",
        default=1,
        type=int,
        help="Number of times to augment the vocabulary for the individual nodes",
    )

    args.add_argument(
        "-x1",
        "--augment_neighborhoods",
        default=1,
        type=int,
        help="Number of times to augment the vocabulary for the tree neighborhoods",
    )

    args.add_argument(
        "-x2",
        "--augment_pairwise_relations",
        default=1,
        type=int,
        help="Number of times to augment the vocabulary for the non-adjacent pairwise relations",
    )

    args.add_argument(
        "-x3",
        "--augment_direct_edges",
        default=1,
        type=int,
        help="Number of times to augment the vocabulary for the direct edges",
    )

    args.add_argument(
        "-x4",
        "--augment_mutually_exclusive_relations",
        default=1,
        type=int,
        help="Number of times to augment the vocabulary for the mutually exclusive relations",
    )

    args.add_argument(
        "-x5",
        "--augment_tree_structure",
        default=1,
        type=int,
        help="Number of times to augment the vocabulary for the tree structure",
    )

    args.add_argument(
        "-x6",
        "--augment_root_child_relations",
        default=1,
        type=int,
        help="Number of times to augment the vocabulary for the root-child node relations",
    )

    args.add_argument(
        "-rlabel",
        "--root_label",
        default=0,
        type=int,
        help="Label of the neutral node (used for discarding certain node relations)",
    )

    args.add_argument(
        "-ilabel",
        "--ignore_label",
        type=int,
        help="Label to be ignored when matching individual nodes or pairwise relations (usecase: ignoring neutral clones)",
    )

    args.add_argument(
        "-threshold",
        "--heatmap_contrast_threshold",
        default=0.4,
        type=float,
        help="Numerical threshold in the range 0 and 1 indicating the cutoff for the hierarhical clustering "
        "w.r.t. the maximum cosine distance between the samples inside each cluster",
    )

    args.add_argument("--use_package", action="store_true")
    args.add_argument("--no_use_package", dest="use_package", action="store_false")
    args.set_defaults(use_package=False)

    args.add_argument("--remove_unique_words", action="store_true")
    args.add_argument(
        "--no_remove_unique_words",
        dest="remove_unique_words",
        action="store_false",
    )
    args.set_defaults(remove_unique_words=True)

    args.add_argument("--generate_heatmaps", action="store_true")
    args.add_argument(
        "--no_generate_heatmaps", dest="generate_heatmaps", action="store_false"
    )
    args.set_defaults(generate_heatmaps=True)

    args.add_argument("--build_voabulary", action="store_true")
    args.add_argument(
        "--no_build_vocabulary", dest="build_vocabulary", action="store_false"
    )
    args.set_defaults(build_vocabulary=True)

    args.add_argument(
        "-t", "--word_tag", default="", help="Tag for the vocabulary words"
    )

    return args.parse_args()


if __name__ == "__main__":
    args = parse_args()
    main(args)
